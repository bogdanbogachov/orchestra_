experiment: "cc_custom_head_test"

evaluation:
  head: "custom_head"   # "default_head" | "custom_head"

model:
  use_custom_head: true
  pooling_strategy: "mean"
  use_fft: true
  use_default_style: false  # If true, applies linear layer to all tokens first (like default head), then selects last token

  num_labels: 35
  pad_token: "<|reserved_special_token_15|>"
  torch_dtype: "float32"
  device_map: "auto"

paths:
  model: "downloaded_models/downloaded_3_2_1b"
  experiments: "experiments"
  data:
    training: "training_data.json"
    train: "train_data.json"
    test: "test_data.json"

lora:
  r: 4
  lora_alpha: 16
  target_modules:
    - "q_proj"
#    - "k_proj"
    - "v_proj"
#    - "o_proj"
  lora_dropout: 0.1
  bias: "none"

training:
  seed: 42
  max_length: 512
  num_train_epochs: 1
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  learning_rate: 0.00005
  logging_steps: 10
  eval_steps: 25
  eval_strategy: "steps"
  save_strategy: "no"
  load_best_model_at_end: false
  metric_for_best_model: "loss"
  fp16: true

data_processing:
  test_size: 0.2
  random_state: 42
  stratify: true

logging:
  log_dir: "logs"
